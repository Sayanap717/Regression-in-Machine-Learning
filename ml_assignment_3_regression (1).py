# -*- coding: utf-8 -*-
"""ML_Assignment_3-Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Hf18IzXx3FPzxhfvolrZmZi6bAFjfbi
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""## 1) Loading and Preprocessing

##### Load the California Housing dataset using the fetch_california_housing function from sklearn.Convert the dataset into a pandas DataFrame for easier handling.Handle missing values (if any) and perform necessary feature scaling (e.g., standardization).Explain the preprocessing steps you performed and justify why they are necessary for this dataset.
"""

#Load the California Housing dataset using the fetch_california_housing function from sklearn
from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()
housing

df = pd.DataFrame(housing.data, columns=housing.feature_names)
df

df.info() # extract basic information about the dataset

# to find the duplicates in the dataset.
df.duplicated().sum()

"""‚úÖThere is no duplicates in this dataset"""

df.isnull().sum() #to check the missing values in the dataset

"""‚úÖ The dataset contains no missing values"""

df.describe() # analyse the statstical measures of the dataset

"""There is no major difference between mean and median of different columns except 'Population' and 'AveOccup';only minor difference are there.There may be a chance of outliers."""

#checking the probability of outliers
sns.boxplot(data=df)
plt.show()

#checking the skewness of the data
skewness = df.skew()
skewness

"""‚úÖ The data is positively skewed in most of the columns"""

#to find the outliers using IQR method
num=df.select_dtypes("number")
for col in num:
  Q1 = df[col].quantile(0.25)
  Q3 = df[col].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
  print(outliers)
  print("\n")

"""The data have no outliers;So that we got an empty DataFrame for outliers"""

from sklearn.model_selection import train_test_split

housing.target

df["Target"]= housing.target
df

df.isnull().sum()

#to find the correlation between the target and other features
corr = df.corr()
corr

#drawing a heatmap based on correlation
sns.heatmap(corr,annot=True,cmap="Purples")
plt.show()

"""‚úÖ Target has a strong positive correlation with 'MedIc' compared to other features"""

x=df.drop("Target",axis=1)
x

y=df["Target"]
y

#feature scaling using standard scaler
from sklearn.preprocessing import StandardScaler
std_scaler=StandardScaler()
x_scaler=std_scaler.fit_transform(x)
x_scaler

x_train,x_test,y_train,y_test=train_test_split(x,y)

x_train.shape

x_test.shape

y_train.shape

y_test.shape

"""## 2) Regression Algorithm Implementation

### Implement the following regression algorithms:
*   Linear Regression
*   Decision Tree Regressor
*   Random Forest Regressor
*   Gradient Boosting Regressor
*   Support Vector Regressor (SVR)

For each algorithm:
*   Provide a brief explanation of how it works.
*   Explain why it might be suitable for this dataset.
"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR

#linear regression
linreg = LinearRegression()
linreg.fit(x_train, y_train)

"""This tells us that the linear regression model (linreg) to learn the relationship between the features (x_train) and the target variable (y_train) from the training data

##### Linear regression can be a good starting point for this dataset :-
###### It's a relatively simple and interpretable model.
###### It can be effective if there's a linear relationship between the features and the target variable (house price). This was previously verified in the code by plotting a heatmap of the correlations. There is a strong correlation between Median Income ('MedIc') and house price ('Target').
"""

inreg_y_pred = linreg.predict(x_test)
inreg_y_pred

#decision Tree Regressor
dt_reg = DecisionTreeRegressor()
dt_reg.fit(x_train, y_train)

"""A Decision tree is a supervised learning algorithm used for classification and regression tasks.The structure resembles a tree with nodes representing decisions,branches represents choices,and leaves representing outcomes or predictions.

Suitability for the California Housing Dataset: Handles Non-linear Relationships,it can provide insights into which features are most important for predicting house prices,no feature Scaling required
"""

dtreg_y_pred = dt_reg.predict(x_test)
dtreg_y_pred

#random Forest Regressor
rfreg = RandomForestRegressor()
rfreg.fit(x_train, y_train)

"""* Random Forest is an ensemble learning method that builds multiple Decision Tress and averages their predictions to improve accuracy and reduce overfitting.Each tree is built on a random subset of data and features.
* Suitability : Reduces overlifting compared to a single decision Tree.Handles non-linear relationships effectively.


"""

rfreg_y_pred = rfreg.predict(x_test)
rfreg_y_pred

#Gradient Boosting Regressor
gbreg = GradientBoostingRegressor()
gbreg.fit(x_train, y_train)

"""

* Gradient Boosting is another method that builds Decision Trees sequentially,where each new tree corrects the errors of the previous one.It minimizes the loss function by literaly improving predictions.
* Suitability : More accurate than Random Forest in many cases.Works well with complex and structured datasets.

"""

gbreg_y_pred = gbreg.predict(x_test)
gbreg_y_pred

#Support Vector Regressor (SVR)
svr = SVR()
svr.fit(x_train, y_train)

"""* SVC uses a set of labeled training examples to find a decision boundary that
 seperates the data points into different classes.The decision boundary is represented as a linear function, and the goal is to find the boundary that maximizes the seperation between the classes.
* SVC is commonly used in image recoginition,text classification.


"""

svreg_y_pred = svr.predict(x_test)
svreg_y_pred

y_test

"""## 3) **Model Evaluation and Comparison**

### 1) Evaluate the performance of each algorithm using the following metrics

###### Mean Squared Error (MSE)
###### Mean Absolute Error (MAE)
###### R-squared Score (R¬≤)

##### Compare the results of all models and identify:
##### a) The best-performing algorithm with justification.
##### b) The worst-performing algorithm with reasoning.
"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# >>Mean Squared Error (MSE)
Inreg_MSE = mean_squared_error(y_test, inreg_y_pred)
dtreg_MSE = mean_squared_error(y_test, dtreg_y_pred)
rfreg_MSE = mean_squared_error(y_test, rfreg_y_pred)
gbreg_MSE = mean_squared_error(y_test, gbreg_y_pred)
svreg_MSE = mean_squared_error(y_test, svreg_y_pred)

print("Mean Squared Error-Linear Regression (MSE):",Inreg_MSE)
print("Mean Squared Error-Decision Tree Regressor (MSE):",dtreg_MSE)
print("Mean Squared Error-Random Forest Regressor (MSE):",rfreg_MSE)
print("Mean Squared Error-Gradient Boosting Regressor (MSE):",gbreg_MSE)
print("Mean Squared Error-Support Vector Regressor (MSE):",svreg_MSE)

# >> Mean Absolute Error(MAE)
Inreg_MAE = mean_absolute_error(y_test,inreg_y_pred)
dtreg_MAE = mean_absolute_error(y_test,dtreg_y_pred)
rfreg_MAE = mean_absolute_error(y_test,rfreg_y_pred)
gbreg_MAE = mean_absolute_error(y_test,gbreg_y_pred)
svreg_MAE = mean_absolute_error(y_test,svreg_y_pred)

print("Mean Squared Error-Linear Regression (MSE):",Inreg_MAE)
print("Mean Squared Error-Decision Tree Regressor (MSE):",dtreg_MAE)
print("Mean Squared Error-Random Forest Regressor (MSE):",rfreg_MAE)
print("Mean Squared Error-Gradient Boosting Regressor (MSE):",gbreg_MAE)
print("Mean Squared Error-Support Vector Regressor (MSE):",svreg_MAE)

# >> R-squared Score (R¬≤)
Inreg_r2_score = r2_score(y_test,inreg_y_pred)
dtreg_r2_score = r2_score(y_test,dtreg_y_pred)
rfreg_r2_score = r2_score(y_test,rfreg_y_pred)
gbreg_r2_score = r2_score(y_test,gbreg_y_pred)
svreg_r2_score = r2_score(y_test,svreg_y_pred)

print("R-squared Score-Linear Regression :",Inreg_r2_score)
print("R-squared Score-Decision Tree Regressor :",dtreg_r2_score)
print("R-squared Score-Random Forest Regressor :",rfreg_r2_score)
print("R-squared Score-Gradient Boosting Regressor :",gbreg_r2_score)
print("R-squared Score-Support Vector Regressor :",svreg_r2_score)

# Compare the results of all models and identify :
predictions = {
    'Linear Regression': inreg_y_pred,
    'Decision Tree Regressor': dtreg_y_pred,
    'Random Forest Regressor': rfreg_y_pred,
    'Gradient Boosting Regressor': gbreg_y_pred,
    'Support Vector Regressor': svreg_y_pred
}

#Initialize a dictionary to store metrics
results = {
    'Model': [],
    'MSE': [],
    'MAE': [],
    'R-squared': []
}

# Compute metrics for each model
for model_name, y_pred in predictions.items():
    results['Model'].append(model_name)
    results['MSE'].append(mean_squared_error(y_test, y_pred))
    results['MAE'].append(mean_absolute_error(y_test, y_pred))
    results['R-squared'].append(r2_score(y_test, y_pred))
results

#Convert results to a DataFrame
df_results = pd.DataFrame(results)
df_results

#sort by R_squared Score
results=df_results.sort_values(by="R-squared",ascending=False)
results

"""The highest R^2 score is 0.808101 (Random Forest Regressor).
The next highest is 0.793593 (Gradient Boosting Regressor).
The values continue to decrease correctly down to -0.030898 (Support Vector Regressor).

*   Mean Squared Error (MSE) - has the lowest MSE (0.253929), indicating the
   best performance.
*   Mean Absolute Error (MAE) - Random Forest Regressor has the lowest MAE (0.331554), meaning it has the most accurate predictions on average.
*   R-squared Score (R^2) - Highest ùëÖ^2 score: 0.808101 (Random Forest Regressor) ‚Üí Best model.

##### ‚úÖ From these results ,we can conclude that the "Random Forest Regressor" is the best performing algoritham since R-squared Score (R^2) score of Random Forest Regressor is higher one.

##### ‚úÖ Mean Squared Error (MSE) and Mean Absolute Error (MAE) are also lower compared to other models which shows better performance and accuracy.

##### ‚úÖThe worst-performing algorithm from the above data, we can say that it is "Support Vector Regressor" model.It has the lowest R-square score,hghest Mean Square Error(MSE) and Mean Absolute Error(MAE) values, which shows worst performance and poor accuracy.
"""